{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitetura dos Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divisão do texto em unidades menores, referenciadas por IDs, de forma a serem processáveis pelo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por palavra: cada palavra é um token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por subpalavra: divide as palavras em morfemas (radical, afixos, etc) ou nro de caracteres fixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por caractere: cada caractere é um token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processo de Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalização: converte o texto para uma forma canônica (tira acentos, lower case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Divisão do texto em tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Converte cada token para um ID: de acordo com um vocabulário pré-definido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tokenization](img/tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "São representações vetoriais densas de palavras ou tokens em um espaço de alta dimensionalidade. Preserva as relações semânticas e contextuais entre as palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings: cada palavra é um vetor, exemplo: Word2Vec; palavras semelhantes produzem vetores próximos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subword embeddings: utilizado pelo BERT e GPT, cada token é um vetor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positional embedding: captura a informação da localização do token no texto (já que os Transformers não guardam a ordem das palavras), e soma essa informação aos outros tipos de embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word2vec](img/word2vec.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processo de Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inicialização aleatória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Treinamento: os embeddings são ajustados por backpropagation para capturar o significado de cada palavra/token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Contextualização: nas camadas subsequentes do Transformer, os embeddings são ajustados para capturar o contexto completo da frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o próprio nome diz, é um mecanismo que permite processar todos os tokens simultaneamente no input, de forma a treinar o modelo para \"focar\" nas relações mais importantes para montar um contexto (Self-Attention). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, o conceito de Self-Attention foi expandido para Multi-Head Attention, que é uma série de mecanismos de atenção onde cada \"cabeça\" tenta aprender relações diferentes entre os tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processo de Atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cálculo dos scores de atenção:\n",
    "   - Q (query): representa o token atual\n",
    "   - K (key): representa os outros tokens em que estamos \"atentos\"\n",
    "   - V (value): representa os valores dos tokens\n",
    "   - calcula o produto escalar entre Q e K para obter a relevância, depois multiplica por V para obter a saída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Softmax: normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Atualiza o valor do token ponderado pelo valor do softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Attention](img/attention.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "São redes neurais densas tradicionais, que ajudam a transformar as representações intermediárias de cada token introduzindo não-linearidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processo de Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$FFN(x)=max(0,xW_1​+b_1​)W_2​+b_2$ \n",
    "\n",
    "(considerando função de ativação ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FFN](img/ffn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder e Decoder"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
