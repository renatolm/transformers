{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O básico sobre Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento de texto na era pré-Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes da arquitetura dos Transformers ser introduzida, as tarefas de processamento de texto costumavam ser realizando RNNs, devido à natureza dos problemas de input/output sequencial e à necessidade de memória para entender o contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP com RNNs](img/nlp_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um pequeno avanço ao uso de apenas uma RNN diretamente foi a arquitetura Encoder-Decoder, que permite analisar todo o input (contexto), antes de começar a processar o output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP com Encoder-Decoder](img/nlp_encoder_decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de serem muito bons para séries temporais, esses modelos apresentavam diversas desvantagens para esse tipo de problema de processamento de texto, mas as principais eram a dificuldade de manter contextos longos e complicados na memória, e o processamento sequencial dos inputs que tornava o treinamento dos modelos bastante demorado/custoso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Oil forecasting](img/oil_forecasting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em 2017, a equipe do Google Brain publicou o famoso artigo \"Attention is All You Need\" que introduziu a arquitetura Transformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Attention is all you need](img/attention_is_all_you_need.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse modelo revolucionou o campo de NLP ao usar apenas mecanismos de atenção, sem convoluções nem recorrências."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer](img/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 inovações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mecanismo de atenção: como o próprio nome diz, esse mecanismo permite receber um input inteiro de uma vez e aprender a \"focar\" nas partes mais relevantes ao processar uma palavra (resolve o problema de contextos longos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paralelização: consegue processar as palavras paralelamente ao invés de sequencialmente como nas RNNs (resolve o problema do treinamento custoso).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer learning: prática comum em processamento de imagens, mas não se sabia como fazer para NLP. Até que surgiu o framework ULMFiT..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ULMFiT](img/ulmfit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa o modelo é treinado a predizer a próxima palavra baseado nas palavras anteriores.<br>\n",
    "Usa um corpus de texto qualquer grande o suficiente, como a Wikipedia.<br>\n",
    "Essa etapa também é conhecida como _language modeling_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa, o modelo é refinado treinando num corpus de texto relacionado ao domínio onde vai ser usado, por exemplo textos jurídicos ou médicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, o modelo é treinado para a tarefa específica que ele vai fazer (classificação, análise de sentimento, etc).<br>\n",
    "Como o modelo já aprendeu a \"falar\" a língua, essa etapa precisa de uma quantidade bem menor de dados rotulados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline dos modelos de Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Timeline](img/timeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos significativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT** (OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focou na geração de texto unidirecional, foi pretreinado usando o BookCorpus, um dataset de 7000 livros não-publicados de vários gêneros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bert** (Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usou pré-treinamento bidirecional, olhando para o texto à esquerda e à direita para predizer uma palavra.\n",
    "\n",
    "Exemplo: \"O juíz [MASK] a sentença.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
